{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GLY_5vh2gcH"
      },
      "source": [
        "## 스트리밍 토큰 수 계산하기 (OpenAI, LangChain)\n",
        "\n",
        "### 1. OpenAI\n",
        "\n",
        "이제 OpenAI 스트리밍 모드에서도 토큰 수를 확인할 수 있습니다.\n",
        "\n",
        "-- **2024.4 업데이트: [OpenAI Cookbook 링크](https://cookbook.openai.com/examples/how_to_stream_completions#4-how-to-get-token-usage-data-for-streamed-chat-completion-response)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai langchain langchain_openai langchain_community --upgrade -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nXh-O3S5lM4",
        "outputId": "c9227056-b328-4c8a-9ff9-8345583abf0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EVMYYI62gcI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"여기에 API 키 입력\"\n",
        "client = openai.Client()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fAAEwSn2gcI"
      },
      "source": [
        "`stream_options={\"include_usage\": True}`를 추가하면 마지막 청크에 사용량 정보가 전달됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWp5SQm12gcI",
        "outputId": "9ee96638-f24b-4e57-bac9-2e39efaf150a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here's a classic English joke about potatoes:\n",
            "\n",
            "**Joke:**\n",
            "Why do potatoes make good detectives?\n",
            "Because they keep their eyes peeled!\n",
            "\n",
            "**Direct Translation:**\n",
            "왜 감자는 좋은 탐정이 될까요?\n",
            "왜냐하면 그들은 눈을 벗기고 있기 때문이에요!\n",
            "\n",
            "**Explanation in Korean:**\n",
            "이 농담은 영어 단어의 중의성을 이용한 것입니다. \"Keep their eyes peeled\"라는 표현은 \"눈을 벗기다\"라는 직역과 \"눈을 크게 뜨고 주의 깊게 보다\"라는 관용적 의미를 동시에 가지고 있습니다. 감자는 껍질을 벗길 때 눈(싹)이 드러나기 때문에, 이 표현이 감자와 관련된 농담으로 사용된 것입니다. 영어 사용자에게는 이중 의미가 재미있게 다가옵니다.\n",
            "\n",
            "\n",
            "CompletionUsage(completion_tokens=178, prompt_tokens=39, total_tokens=217)\n"
          ]
        }
      ],
      "source": [
        "topic = 'Potatoes'\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o',\n",
        "    messages=[\n",
        "        {'role': 'user', 'content': f\"\"\"tell me a english joke about {topic},\n",
        "also, explain in Korean why it is fun for english-speakers.\n",
        "Include direct translation of the joke.\"\"\"}\n",
        "    ],\n",
        "    temperature=0.2,\n",
        "    stream=True, # 스트리밍 옵션 추가\n",
        "    stream_options={\"include_usage\": True}\n",
        "    # 맨 마지막 청크에 사용량 정보를 포함\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    if chunk.choices!=[]:\n",
        "        token = chunk.choices[0].delta.content\n",
        "        if token: print(token,end='',flush=True)\n",
        "\n",
        "print('\\n\\n')\n",
        "print(chunk.usage)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjPhz_IC2gcJ"
      },
      "source": [
        "<br><br>\n",
        "<br><br>\n",
        "\n",
        "### 2. LangChain\n",
        "랭체인도 같은 방식으로 `stream_options={\"include_usage\": True}` 를 통해 토큰 수를 구할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awjiQS6l2gcJ"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ollgFfc2gcJ"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"tell me a english joke about {topic},\n",
        "                also, explain in Korean why it is fun for english-speakers.\n",
        "                Include direct translation of the joke.\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLO_vpkt2gcJ",
        "outputId": "687548af-e81a-4d41-bfe0-6bdab3121471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English joke: Why did the walnut go to the party? Because it was a little nutty!\n",
            "\n",
            "Korean explanation: 영어권 사람들에게 재미있는 이유는 \"nutty\"가 \"조금 미친\"이라는 뜻이 있어서, walnut이 조금 미친 것처럼 파티에 가서 재미있게 보낸다는 농담입니다.\n",
            "\n",
            "Direct translation: \"호두가 왜 파티에 갔나요? 약간 미친 것 같아서요!\"\n",
            "\n",
            "\n",
            "{'input_tokens': 43, 'output_tokens': 146, 'total_tokens': 189}\n"
          ]
        }
      ],
      "source": [
        "result = llm.stream(prompt.format(topic='walnet'), stream_options={\"include_usage\": True})\n",
        "for chunk in result:\n",
        "    print(chunk.content, end='', flush=True)\n",
        "\n",
        "print('\\n\\n')\n",
        "print(chunk.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmsJ_bTd2gcJ"
      },
      "source": [
        "<br><br>\n",
        "만약 LCEL Chain에서 이를 활용하고 싶다면, 아래와 같이 설정해줄 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlBGMP342gcK"
      },
      "outputs": [],
      "source": [
        "llm2 = ChatOpenAI(\n",
        "    model=\"gpt-4o\",\n",
        "    model_kwargs={\"stream_options\": {\"include_usage\": True}},\n",
        ")\n",
        "\n",
        "chain2 = prompt | llm2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ofk3W4j32gcK",
        "outputId": "0fd9162f-01b3-4bec-8767-924d1cc392fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Joke:**\n",
            "Why did the chicken join a band?\n",
            "Because it had the drumsticks!\n",
            "\n",
            "**Direct Translation:**\n",
            "왜 닭이 밴드에 들어갔을까요?\n",
            "왜냐하면 닭다리가 있었기 때문이에요!\n",
            "\n",
            "**Explanation in Korean:**\n",
            "이 농담은 영어권 사람들에게 재미있는 이유는 'drumsticks'라는 단어에 있습니다. 'Drumsticks'는 두 가지 의미를 가지고 있어요. 첫째, 드럼을 칠 때 사용하는 드럼 스틱을 의미하고, 둘째, 닭다리를 의미합니다. 그래서 이 농담은 닭이 '드럼 스틱'을 가지고 있기 때문에 밴드에 들어갔다는 두 가지 의미를 가진 말장난입니다. 이러한 언어 유희가 영어권 사람들에게 웃음을 줍니다.\n",
            "\n",
            "\n",
            "{'input_tokens': 40, 'output_tokens': 175, 'total_tokens': 215}\n"
          ]
        }
      ],
      "source": [
        "result = chain2.stream({'topic':'chickens'})\n",
        "for chunk in result:\n",
        "    print(chunk.content, end='', flush=True)\n",
        "\n",
        "print('\\n\\n')\n",
        "print(chunk.usage_metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjVtlPev2gcK"
      },
      "source": [
        "<br><br>\n",
        "<br><br>\n",
        "\n",
        "### 3. LangChain Callback을 이용한 방법   \n",
        "기존의 `StdOutCallbackHandler`는 스트리밍 환경에서 동작하지 않기 때문에, 비슷한 콜백을 만들어서 활용할 수도 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg2xtp4Z2gcK",
        "outputId": "50d2b2a0-3d9e-4c59-ae28-d6530a7da9ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Joke: Why did the ice cream break up with the sundae? Because it couldn't find a cone-nection!\n",
            "\n",
            "Explanation in Korean: 영어권 사람들에게 이 농담이 재미있는 이유는 \"cone-nection\"이라는 말장난 때문입니다. \"Cone-nection\"은 \"connection\"(연결)과 \"cone\"(콘)을 합쳐 만든 신조어로, 아이스크림이 선데이와 헤어진 이유를 연결고리(연결)를 찾지 못해서라는 농담입니다.\n",
            "\n",
            "Translation: 아이스크림이 선데이와 헤어진 이유는 뭘까요? 콘과 연결고리를 찾지 못해서였어요!\n",
            "\n",
            "\n",
            "Tokens Used: 199\n",
            "\tPrompt Tokens: 35\n",
            "\tCompletion Tokens: 164\n",
            "Total Cost (USD): $0.002635\n",
            "\tPrompt Cost (USD): $0.000175\n",
            "\tCompletion Cost (USD): $0.002460\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.callbacks import BaseCallbackHandler\n",
        "from typing import Dict, Any, List\n",
        "import tiktoken\n",
        "\n",
        "from langchain_community.callbacks.openai_info import get_openai_token_cost_for_model\n",
        "# 모델별 Cost를 계산하는 함수\n",
        "\n",
        "model_name = 'gpt-4o'\n",
        "\n",
        "\n",
        "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "class TokenMetricsCallbackHandler(BaseCallbackHandler):\n",
        "\n",
        "    prompt_tokens = 0\n",
        "    completion_tokens = 0\n",
        "    completion_cost = 0\n",
        "    prompt_cost = 0\n",
        "    total_cost = 0.0\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return (\n",
        "            f\"Tokens Used: {self.prompt_tokens + self.completion_tokens}\\n\"\n",
        "            f\"\\tPrompt Tokens: {self.prompt_tokens}\\n\"\n",
        "            f\"\\tCompletion Tokens: {self.completion_tokens}\\n\"\n",
        "            f\"Total Cost (USD): ${self.total_cost:f}\\n\"\n",
        "            f\"\\tPrompt Cost (USD): ${self.prompt_cost:f}\\n\"\n",
        "            f\"\\tCompletion Cost (USD): ${self.completion_cost:f}\"\n",
        "\n",
        "        )\n",
        "\n",
        "    async def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:\n",
        "        \"\"\"프롬프트 토큰 개수 계산\"\"\"\n",
        "        self.prompt_tokens += len(enc.encode(prompts[0]))\n",
        "\n",
        "    async def on_llm_new_token(self, token: str, **kwargs):\n",
        "        \"\"\"출력 토큰 개수 1씩 증가\"\"\"\n",
        "        self.completion_tokens += 1\n",
        "\n",
        "    async def on_llm_end(self, response, **kwargs):\n",
        "        \"\"\"OpenAI 모델 이름으로부터 토큰 비용 계산\"\"\"\n",
        "        self.prompt_cost += get_openai_token_cost_for_model(model_name, self.prompt_tokens)\n",
        "        self.completion_cost += get_openai_token_cost_for_model(model_name, self.completion_tokens, is_completion=True)\n",
        "        self.total_cost = self.prompt_cost + self.completion_cost\n",
        "\n",
        "\n",
        "callback_handler = TokenMetricsCallbackHandler()\n",
        "\n",
        "config = {\n",
        "    'callbacks':[callback_handler]\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"tell me a english joke about {topic},\n",
        "                also, explain in Korean why it is fun for english-speakers.\n",
        "                Include translation of the joke.\"\"\")\n",
        "\n",
        "chain = prompt | llm\n",
        "result = chain.stream({'topic':'Ice Cream'}, config=config)\n",
        "for chunk in result:\n",
        "    print(chunk.content,end='',flush=True)\n",
        "\n",
        "print('\\n\\n')\n",
        "print(callback_handler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qlvmOMd2gcK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "test",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}